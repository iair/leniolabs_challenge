{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import TransformComponent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from typing import Sequence\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM...\n",
      "LLM test successful with model: gpt-4o-mini\n",
      "Response: Hello! How can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing LLM...\")\n",
    "utils.test_llm(utils.get_api_key(name='openai'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Embedding Model...\n",
      "Embedding test successful!\n",
      "Embedding shape: 3072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTesting Embedding Model...\")\n",
    "utils.test_embeddings(utils.get_api_key(name='openai'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de Llama Index\n",
    "\n",
    "Modelos y embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(api_key=utils.get_api_key(name='openai'),model=\"gpt-4o-mini\", temperature=0.7, max_tokens=1024)\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=utils.get_api_key(name='openai'), model=\"text-embedding-3-large\", embed_batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar documentos del RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: a8b23985-3bcd-4eba-9954-07436a1de5c3\n",
      "Text: What is Data Science? 3 •When you start with the question you\n",
      "often discover that you need to collect new data or design an experi-\n",
      "ment to confirm you are getting the right answer. •It is easy to\n",
      "discover structure or networks in a data set. There will always be\n",
      "correlations for a thousand reasons if you collect enough data.\n",
      "Understanding wheth...\n"
     ]
    }
   ],
   "source": [
    "book = SimpleDirectoryReader(\"../data/books\").load_data()\n",
    "len(book)\n",
    "print(book[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de ingesta\n",
    "\n",
    "* Crea chunks de un máximo de 512 tokens con 40 tokens de solape entre ellos\n",
    "* Se crean los embeddings asociados a los chunks extraídos\n",
    "* El resultado es la construcción de nodos\n",
    "* Se limpian los nodos\n",
    "* IngestionPipeline tiene la restricción que trabaja solo con clases que heredan de TransformerComponent\n",
    "* Referencia : https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations/#custom-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextCleaner(TransformComponent):\n",
    "    \"\"\"\n",
    "    A transformation component that cleans text by removing non-alphanumeric characters.\n",
    "\n",
    "    This component takes in a list of nodes, removes any non-alphanumeric characters\n",
    "    (except for spaces) from the text of each node, and returns the cleaned nodes.\n",
    "\n",
    "    Args:\n",
    "        nodes (list): A list of nodes to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of cleaned nodes.\n",
    "    \"\"\"\n",
    "    def __call__(self, nodes, **kwargs):\n",
    "        for node in nodes:\n",
    "            node.text = re.sub(r\"[^0-9A-Za-z ]\", \"\", node.text)\n",
    "        return nodes\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=40),\n",
    "        TextCleaner(),\n",
    "    ],\n",
    ")\n",
    "nodes = pipeline.run(documents=book)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea el índice de embeddings en memoria\n",
    "\n",
    "* Cómo construyen el índica es un mecanismo interno de la librería\n",
    "* Lo importante es que luego este índice se usa para encontrar los chunks más relevantes a la consulta del usuario por similitud de coseno\n",
    "* https://www.reddit.com/r/LocalLLaMA/comments/1bvo5l4/the_more_i_use_llamaindex_the_less_i_like_it/\n",
    "* referencia: https://docs.llamaindex.ai/en/stable/understanding/loading/loading/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lenolabs_challenge_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating embeddings: 100%|██████████| 157/157 [00:16<00:00,  9.50it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex(\n",
    "        nodes=nodes,\n",
    "        show_progress=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación del RAG usando métodos que vienen en la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specific stages of a data science project are as follows:\n",
      "\n",
      "1. Question\n",
      "2. Exploratory data analysis\n",
      "3. Formal modeling\n",
      "4. Interpretation\n",
      "5. Communication\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"which are the Specific stages of a data science project according to the book?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del Chat\n",
    "\n",
    "* Se usa como referencia esto: https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = vector_index.as_chat_engine(chat_mode=\"openai\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Hi\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Hi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: which are the Specific stages of a data science project?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\":\"What are the specific stages of a data science project?\"}\n",
      "Got output: The specific stages of a data science project are:\n",
      "\n",
      "1. Question\n",
      "2. Exploratory data analysis\n",
      "3. Formal modeling\n",
      "4. Interpretation\n",
      "5. Communication\n",
      "========================\n",
      "\n",
      "The specific stages of a data science project are:\n",
      "\n",
      "1. **Question**: Defining the problem or question to be addressed.\n",
      "2. **Exploratory Data Analysis**: Analyzing the data to understand its structure, patterns, and insights.\n",
      "3. **Formal Modeling**: Building and validating models using statistical and machine learning techniques.\n",
      "4. **Interpretation**: Interpreting the results of the models and understanding their implications.\n",
      "5. **Communication**: Presenting the findings in a clear and actionable manner to stakeholders.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"which are the Specific stages of a data science project?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: thanks, and now can you translate it to spanish?\n",
      "Of course! Here’s the translation:\n",
      "\n",
      "Las etapas específicas de un proyecto de ciencia de datos son:\n",
      "\n",
      "1. **Pregunta**: Definir el problema o la pregunta que se va a abordar.\n",
      "2. **Análisis exploratorio de datos**: Analizar los datos para entender su estructura, patrones e insights.\n",
      "3. **Modelado formal**: Construir y validar modelos utilizando técnicas estadísticas y de aprendizaje automático.\n",
      "4. **Interpretación**: Interpretar los resultados de los modelos y entender sus implicaciones.\n",
      "5. **Comunicación**: Presentar los hallazgos de manera clara y accionable a las partes interesadas.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"thanks, and now can you translate it to spanish?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenolabs_challenge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
