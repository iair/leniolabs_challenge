{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM...\n",
      "LLM test successful with model: gpt-4o-mini\n",
      "Response: Hello! How can I assist you today?\n",
      "\n",
      "Testing Embedding Model...\n",
      "Embedding test successful!\n",
      "Embedding shape: 3072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import openai\n",
    "import numpy as np\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import TransformComponent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings,StorageContext\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from typing import Sequence\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "import pickle\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import utils\n",
    "print(\"Testing LLM...\")\n",
    "utils.test_llm(utils.get_api_key(name='openai'))\n",
    "print(\"\\nTesting Embedding Model...\")\n",
    "utils.test_embeddings(utils.get_api_key(name='openai'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de Llama Index\n",
    "\n",
    "Modelos y embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(api_key=utils.get_api_key(name='openai'),model=\"gpt-4o-mini\", temperature=0.5, max_tokens=1024)\n",
    "Settings.embed_model = OpenAIEmbedding(api_key=utils.get_api_key(name='openai'), model=\"text-embedding-3-large\", embed_batch_size=10,dimensions=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar documentos del RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 78b07c04-4f7a-445c-bb12-49d8d17ec8b5\n",
      "Text: What is Data Science? 3 •When you start with the question you\n",
      "often discover that you need to collect new data or design an experi-\n",
      "ment to confirm you are getting the right answer. •It is easy to\n",
      "discover structure or networks in a data set. There will always be\n",
      "correlations for a thousand reasons if you collect enough data.\n",
      "Understanding wheth...\n"
     ]
    }
   ],
   "source": [
    "book = SimpleDirectoryReader(\"../data/books\").load_data()\n",
    "len(book)\n",
    "print(book[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de ingesta\n",
    "\n",
    "* Crea chunks de un máximo de 512 tokens con 40 tokens de solape entre ellos\n",
    "* Se crean los embeddings asociados a los chunks extraídos\n",
    "* El resultado es la construcción de nodos\n",
    "* Se limpian los nodos\n",
    "* IngestionPipeline tiene la restricción que trabaja solo con clases que heredan de TransformerComponent\n",
    "* Referencia : https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations/#custom-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextCleaner(TransformComponent):\n",
    "    \"\"\"\n",
    "    A transformation component that cleans text by removing non-alphanumeric characters.\n",
    "\n",
    "    This component takes in a list of nodes, removes any non-alphanumeric characters\n",
    "    (except for spaces) from the text of each node, and returns the cleaned nodes.\n",
    "\n",
    "    Args:\n",
    "        nodes (list): A list of nodes to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of cleaned nodes.\n",
    "    \"\"\"\n",
    "    def __call__(self, nodes, **kwargs):\n",
    "        for node in nodes:\n",
    "            node.text = re.sub(r\"[^0-9A-Za-z ]\", \"\", node.text)\n",
    "        return nodes\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1000, chunk_overlap=120),\n",
    "        TextCleaner(),\n",
    "    ],\n",
    ")\n",
    "nodes = pipeline.run(documents=book)\n",
    "# Guardar los nodos en un archivo para reutilizarlos\n",
    "with open('../data/models/processed_nodes.pkl', 'wb') as f:\n",
    "    pickle.dump(nodes, f)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea el índice de embeddings en memoria\n",
    "\n",
    "* Cómo construyen el índica es un mecanismo interno de la librería\n",
    "* Lo importante es que luego este índice se usa para encontrar los chunks más relevantes a la consulta del usuario por similitud de coseno\n",
    "* https://www.reddit.com/r/LocalLLaMA/comments/1bvo5l4/the_more_i_use_llamaindex_the_less_i_like_it/\n",
    "* referencia: https://docs.llamaindex.ai/en/stable/understanding/loading/loading/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lenolabs_challenge_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating embeddings: 100%|██████████| 153/153 [00:11<00:00, 13.52it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex(\n",
    "        nodes=nodes,\n",
    "        show_progress=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.storage_context.persist('../data/models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del RAG\n",
    "\n",
    "* Usamos una consulta directa a lo que generamos el RAG y lo comparamos con la respuesta que da usando el motor de chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos las respuestas haciendo consultas con el query engine y luego con el conocimiento primario de GPT-4o-mini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query responses have been saved to query_responses.txt\n",
      "Chat responses have been saved to chat_responses.txt\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"1. Which are the specific stages of a data science project?\",\n",
    "    \"2. Which are the roles in a data science team?\",\n",
    "    \"3. Define success in data science experiments.\",\n",
    "    \"3. Which are the software engineering principles for data science?\"\n",
    "]\n",
    "query_responses = {}\n",
    "chat_responses = {}\n",
    "query_engine = vector_index.as_query_engine()\n",
    "for i, question in enumerate(questions, 1):\n",
    "    query_responses[f\"question_{i}\"] = query_engine.query(question)\n",
    "    \n",
    "chat_engine = vector_index.as_chat_engine(chat_mode=\"openai\", verbose=False)\n",
    "response = chat_engine.chat(\"Hi, please answer using your primary knowledge. Don't use alist inside of a question, use only sentences.Start every answer with the word \\\"Answer:\\\" and the number of the question.\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    chat_responses[f\"question_{i}\"] = chat_engine.chat(question)\n",
    "\n",
    "try:\n",
    "    # Guardar las respuestas en un archivo\n",
    "    utils.save_query_responses(\"../data/output/query_responses.txt\", query_responses)\n",
    "    # Imprimir confirmación\n",
    "    print(\"Query responses have been saved to query_responses.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"Query response saving failed: {str(e)}\")\n",
    "    \n",
    "try:\n",
    "    # Guardar las respuestas en un archivo\n",
    "    utils.save_chat_responses(\"../data/output/chat_responses.txt\", chat_responses)\n",
    "    # Imprimir confirmación\n",
    "    print(\"Chat responses have been saved to chat_responses.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"Chat response saving failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluaciones guardadas en ../data/output/response_evaluation.txt\n"
     ]
    }
   ],
   "source": [
    "# Configura tu clave de API de OpenAI\n",
    "openai.api_key = utils.get_api_key(name=\"openai\")\n",
    "# Leer respuestas desde los archivos\n",
    "query_responses = utils.load_responses_from_txt(\"../data/output/query_responses.txt\")\n",
    "chat_responses = utils.load_responses_from_txt(\"../data/output/chat_responses.txt\")\n",
    "\n",
    "# Evaluar respuestas usando ChatGPT\n",
    "evaluations = utils.compare_responses_with_chatgpt(\n",
    "    client=openai, \n",
    "    query_responses=query_responses, \n",
    "    chat_responses=chat_responses, \n",
    "    model=\"gpt-4\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024\n",
    ")\n",
    "output_file = \"../data/output/response_evaluation.txt\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "utils.save_evaluations_to_file(output_file, evaluations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenolabs_challenge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
