{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import TransformComponent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from typing import Sequence\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "import os\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de Llama Index\n",
    "\n",
    "Modelos y embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.7, max_tokens=1024)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",embed_batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar documentos del RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = SimpleDirectoryReader(\"../data\").load_data()\n",
    "len(book)\n",
    "print(book[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de ingesta\n",
    "\n",
    "* Crea chunks de un máximo de 512 tokens con 40 tokens de solape entre ellos\n",
    "* Se crean los embeddings asociados a los chunks extraídos\n",
    "* El resultado es la construcción de nodos\n",
    "* Se limpian los nodos\n",
    "* IngestionPipeline tiene la restricción que trabaja solo con clases que heredan de TransformerComponent\n",
    "* Referencia : https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations/#custom-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner(TransformComponent):\n",
    "    \"\"\"\n",
    "    A transformation component that cleans text by removing non-alphanumeric characters.\n",
    "\n",
    "    This component takes in a list of nodes, removes any non-alphanumeric characters\n",
    "    (except for spaces) from the text of each node, and returns the cleaned nodes.\n",
    "\n",
    "    Args:\n",
    "        nodes (list): A list of nodes to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of cleaned nodes.\n",
    "    \"\"\"\n",
    "    def __call__(self, nodes, **kwargs):\n",
    "        for node in nodes:\n",
    "            node.text = re.sub(r\"[^0-9A-Za-z ]\", \"\", node.text)\n",
    "        return nodes\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=40),\n",
    "        TextCleaner(),\n",
    "    ],\n",
    ")\n",
    "nodes = pipeline.run(documents=book)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea el índice de embeddings en memoria\n",
    "\n",
    "* Cómo construyen el índica es un mecanismo interno de la librería\n",
    "* Lo importante es que luego este índice se usa para encontrar los chunks más relevantes a la consulta del usuario por similitud de coseno\n",
    "* https://www.reddit.com/r/LocalLLaMA/comments/1bvo5l4/the_more_i_use_llamaindex_the_less_i_like_it/\n",
    "* referencia: https://docs.llamaindex.ai/en/stable/understanding/loading/loading/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(\n",
    "        nodes=nodes,\n",
    "        show_progress=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación del RAG usando métodos que vienen en la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"What is the messy middle and how does it differ from traditional models of the consumer decision-making process?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del Chat\n",
    "\n",
    "* Se usa como referencia esto: https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = vector_index.as_chat_engine(chat_mode=\"openai\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"Hi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"What is the messy middle and how does it differ from traditional models of the consumer decision-making process?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"thanks, and now can you translate it to spanish?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenolabs_challenge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
